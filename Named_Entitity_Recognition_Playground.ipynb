{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named_Entitity_Recognition_Playground.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdlKofTCGD99"
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "from spacy.util import minibatch, compounding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoA1OToqkcuQ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnUsQprJHMbJ"
      },
      "source": [
        "url = \"https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio\"    \n",
        "html = requests.get(url)\n",
        "html_page = html.content\n",
        "\n",
        "text = BeautifulSoup(html_page, 'html.parser')\n",
        "with open('data.txt','w') as f:\n",
        "  f.write(text.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLOf39LINLDt"
      },
      "source": [
        "with open('data.txt','r') as f:\n",
        "  sentences = []\n",
        "  sentence = []\n",
        "  for line in f:\n",
        "    if line != '\\n':\n",
        "      sentence.append(line)\n",
        "    else:\n",
        "      sentences.append(''.join(sentence))\n",
        "      sentence = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv9Ztz5-k7vs"
      },
      "source": [
        "train_data = []\n",
        "label = []\n",
        "for sentence in sentences:\n",
        "  res = pd.read_csv(StringIO(sentence),delimiter='\\t',header=None)\n",
        "  res.columns = ['Tag','Word']\n",
        "  res['Word_Length'] = res['Word'].apply(lambda x: len(str(x)))\n",
        "  res['Word_Start'] = ((res['Word_Length']+1).cumsum()).shift(1,fill_value=0)\n",
        "  res['Word_End'] = res['Word_Start'] + res['Word_Length']\n",
        "  words = res['Word'].values.tolist()\n",
        "  if len(words)==1:\n",
        "    text = str(words[0])\n",
        "  else:\n",
        "    text = ' '.join(words)\n",
        "  list_entities = []\n",
        "  for i in range(res.shape[0]):\n",
        "    tag = res.loc[i,'Tag']\n",
        "    start =  res.loc[i,'Word_Start']\n",
        "    end =  res.loc[i,'Word_End']\n",
        "    if tag not in label:\n",
        "      label.append(tag)\n",
        "    if tag != 'O':\n",
        "      list_entities.append((start,end,tag))\n",
        "  train_data.append((text,{'entities':list_entities}))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bR-wdKOnnJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6330ec0e-7c32-4f0c-c980-25c89dbe3a7d"
      },
      "source": [
        "nlp = spacy.blank('en')\n",
        "ner = nlp.create_pipe('ner')\n",
        "nlp.add_pipe(ner)\n",
        "for i in label:\n",
        "  ner.add_label(i)\n",
        "optimizer = nlp.begin_training()\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    for itn in range(30):\n",
        "        random.shuffle(train_data)\n",
        "        losses = {}\n",
        "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
        "        for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
        "                        losses=losses)\n",
        "        print('Losses', losses)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 21978.942205010993}\n",
            "Losses {'ner': 15454.985520238266}\n",
            "Losses {'ner': 13581.910265806877}\n",
            "Losses {'ner': 12373.159804072391}\n",
            "Losses {'ner': 11634.135050100598}\n",
            "Losses {'ner': 11134.175764689579}\n",
            "Losses {'ner': 10547.605561620534}\n",
            "Losses {'ner': 10291.67309443081}\n",
            "Losses {'ner': 9764.054894283905}\n",
            "Losses {'ner': 9601.554304712074}\n",
            "Losses {'ner': 9186.407680712146}\n",
            "Losses {'ner': 9017.84045280131}\n",
            "Losses {'ner': 8734.428664283116}\n",
            "Losses {'ner': 8548.251742810628}\n",
            "Losses {'ner': 8488.69488681472}\n",
            "Losses {'ner': 8278.635534887526}\n",
            "Losses {'ner': 8221.475480006113}\n",
            "Losses {'ner': 8040.667739010552}\n",
            "Losses {'ner': 7634.014516340962}\n",
            "Losses {'ner': 7660.055042908127}\n",
            "Losses {'ner': 7487.649212316109}\n",
            "Losses {'ner': 7416.833368636181}\n",
            "Losses {'ner': 7310.52988831355}\n",
            "Losses {'ner': 7234.063362982268}\n",
            "Losses {'ner': 7007.7794007783905}\n",
            "Losses {'ner': 7049.773356238586}\n",
            "Losses {'ner': 6819.635488878275}\n",
            "Losses {'ner': 6877.326699053507}\n",
            "Losses {'ner': 6700.064797591431}\n",
            "Losses {'ner': 6634.047073643286}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRPicO_2r8s-",
        "outputId": "83807524-0422-4835-cc3d-a3a519af332e"
      },
      "source": [
        "# Test the trained model\n",
        "test_text = 'a four star restaurant with a bar'\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.label_, ent.text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities in 'a four star restaurant with a bar'\n",
            "B-Rating four\n",
            "I-Rating star\n",
            "B-Amenity bar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUTf7OV-sHRt"
      },
      "source": [
        "# Save model \n",
        "nlp.to_disk('NLP Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-CAm9ErtuAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1163c0fa-acc1-4dca-d4a7-0ede989bfb62"
      },
      "source": [
        "#Load the saved model for inference\n",
        "nlp2 = spacy.load('NLP Training')\n",
        "doc2 = nlp2(test_text)\n",
        "for ent in doc2.ents:\n",
        "  print(ent.label_, ent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B-Rating four\n",
            "I-Rating star\n",
            "B-Amenity bar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLRHXvcn5_5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}